{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids_keep: tensor([2, 4])\n",
      "masked_input: tensor([ 6, 10])\n",
      "____________________________________________________________________________________________________\n",
      "revert: tensor([4, 2, 0, 3, 1])\n",
      "final_input: tensor([ 6., 10.,  0.,  0.,  0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  6.,  0., 10.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITHOUT PADDING\n",
    "import torch\n",
    "sample = 0.5\n",
    "\n",
    "# WE ARE ON DATALOADER STAGE\n",
    "arr1 = torch.tensor([2,4,6,8,10])#.unsqueeze(-1)\n",
    "num_sample = int(len(arr1) * sample)\n",
    "\n",
    "# Index for shuffle and revert\n",
    "noise = torch.rand(arr1.shape)\n",
    "shuffle = torch.argsort(noise, dim=0)\n",
    "revert = torch.argsort(shuffle, dim=0)\n",
    "\n",
    "# Get keep value\n",
    "idx_keep = shuffle[:num_sample]\n",
    "masked_input = torch.gather(arr1, index=idx_keep, dim=0)\n",
    "print(\"ids_keep:\", idx_keep)\n",
    "print(\"masked_input:\", masked_input) # Order of masked input does not matter since we already pos_encoded the sequence\n",
    "print(\"_\"*100)\n",
    "\n",
    "# Revert\n",
    "final_input = torch.cat([masked_input, torch.zeros(len(arr1) - num_sample)])\n",
    "print(\"revert:\", revert)\n",
    "print(\"final_input:\", final_input)\n",
    "torch.gather(final_input, index=revert, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids_keep: tensor([4, 0])\n",
      "masked_input: tensor([10,  2])\n",
      "padded_input: tensor([10.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "____________________________________________________________________________________________________\n",
      "revert: tensor([1, 2, 4, 3, 0, 9, 9, 9, 9, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITH PADDING\n",
    "max_seq_len = 10\n",
    "mask_value = 0\n",
    "padding_idx = max_seq_len\n",
    "\n",
    "sample = 0.5\n",
    "arr1 = torch.tensor([2,4,6,8,10])#.unsqueeze(-1)\n",
    "num_sample = int(len(arr1) * sample)\n",
    "\n",
    "# Index for shuffle and revert\n",
    "noise = torch.rand(arr1.shape)\n",
    "shuffle = torch.argsort(noise, dim=0)\n",
    "revert = torch.argsort(shuffle, dim=0)\n",
    "\n",
    "# Get keep value\n",
    "idx_keep = shuffle[:num_sample]\n",
    "masked_input = torch.gather(arr1, index=idx_keep, dim=0)\n",
    "print(\"ids_keep:\", idx_keep)\n",
    "print(\"masked_input:\", masked_input) # Order of masked input does not matter since we already pos_encoded the sequence\n",
    "\n",
    "# Padding\n",
    "padded_input = torch.concat([masked_input, torch.zeros(max_seq_len - len(masked_input))+mask_value])\n",
    "print(\"padded_input:\", padded_input)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# Revert\n",
    "revert = torch.cat([revert, torch.zeros(max_seq_len-len(revert)) + (max_seq_len-1)]).to(torch.int64)\n",
    "print(\"revert:\", revert)\n",
    "res = torch.gather(padded_input, index=revert, dim=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids_keep: tensor([1])\n",
      "masked_input: tensor([4])\n",
      "--------------------------------------------------\n",
      "ids_keep: tensor([0, 1])\n",
      "masked_input: tensor([2, 4])\n",
      "--------------------------------------------------\n",
      "ids_keep: tensor([3, 1, 2])\n",
      "masked_input: tensor([8, 4, 6])\n",
      "--------------------------------------------------\n",
      "ids_keep: tensor([1, 0, 3, 2])\n",
      "masked_input: tensor([4, 2, 8, 6])\n",
      "--------------------------------------------------\n",
      "padded_input:\n",
      " tensor([[4, 0, 0, 0],\n",
      "        [2, 4, 0, 0],\n",
      "        [8, 4, 6, 0],\n",
      "        [4, 2, 8, 6]])\n",
      "revert:\n",
      " tensor([[1, 0, 4, 4, 4],\n",
      "        [0, 1, 2, 4, 4],\n",
      "        [3, 1, 2, 0, 4],\n",
      "        [1, 0, 3, 2, 4]])\n",
      "____________________________________________________________________________________________________\n",
      "max_seq_len: 5\n",
      "filled_input:\n",
      " tensor([[4., 0., 0., 0., 0., 0.],\n",
      "        [2., 4., 0., 0., 0., 0.],\n",
      "        [8., 4., 6., 0., 0., 0.],\n",
      "        [4., 2., 8., 6., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 4., 0., 0., 0.],\n",
       "        [2., 4., 0., 0., 0.],\n",
       "        [0., 4., 6., 8., 0.],\n",
       "        [2., 4., 6., 8., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MULTIPLE DIMENSTIONS\n",
    "# WITH PADDING\n",
    "mask_value = 0\n",
    "\n",
    "sample = 0.9\n",
    "arr1 = torch.tensor([2,4])\n",
    "arr2 = torch.tensor([2,4,6])\n",
    "arr3 = torch.tensor([2,4,6,8])\n",
    "arr4 = torch.tensor([2,4,6,8,10])\n",
    "\n",
    "arr_li = [arr1, arr2, arr3, arr4]\n",
    "revert_li = []\n",
    "masked_input_li = []\n",
    "max_seq_len = 0\n",
    "\n",
    "for i in range(len(arr_li)):\n",
    "    num_sample = int(len(arr_li[i]) * sample)\n",
    "\n",
    "    # Index for shuffle and revert\n",
    "    noise = torch.rand(arr_li[i].shape)\n",
    "    shuffle = torch.argsort(noise, dim=0)\n",
    "    revert = torch.argsort(shuffle, dim=0)\n",
    "    max_seq_len = len(revert) if len(revert)>max_seq_len else max_seq_len\n",
    "    revert_li.append(revert)\n",
    "    \n",
    "\n",
    "    # Get keep value\n",
    "    idx_keep = shuffle[:num_sample]\n",
    "    masked_input = torch.gather(arr_li[i], index=idx_keep, dim=0)\n",
    "    masked_input_li.append(masked_input)\n",
    "    print(\"ids_keep:\", idx_keep)\n",
    "    print(\"masked_input:\", masked_input) # Order of masked input does not matter since we already pos_encoded the sequence\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Padding\n",
    "padded_input = torch.nn.utils.rnn.pad_sequence(masked_input_li, batch_first=True)\n",
    "revert = torch.nn.utils.rnn.pad_sequence(revert_li, batch_first=True, padding_value=max_seq_len-1)\n",
    "print(\"padded_input:\\n\", padded_input)\n",
    "print(\"revert:\\n\", revert)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# Revert\n",
    "print(\"max_seq_len:\", max_seq_len)\n",
    "filled_input = torch.concat([padded_input, torch.zeros(padded_input.shape[0], (max_seq_len - padded_input.shape[1]+1))], dim=1)\n",
    "print(\"filled_input:\\n\", filled_input)\n",
    "torch.gather(filled_input, index=revert, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# MULTIPLE DIMENSTIONS\n",
    "# WITH PADDING\n",
    "mask_value = 0\n",
    "\n",
    "sample = 0.5\n",
    "arr1 = torch.tensor([2,4])\n",
    "arr2 = torch.tensor([2,4,6])\n",
    "arr3 = torch.tensor([2,4,6,8])\n",
    "arr4 = torch.tensor([2,4,6,8,10])\n",
    "\n",
    "arr_li = [arr1, arr2, arr3, arr4]\n",
    "revert_li = []\n",
    "keep_li = []\n",
    "max_seq_len = 0\n",
    "\n",
    "for i in range(len(arr_li)):\n",
    "    num_sample = int(len(arr_li[i]) * sample)\n",
    "\n",
    "    # Index for shuffle and revert\n",
    "    noise = torch.rand(arr_li[i].shape)\n",
    "    shuffle = torch.argsort(noise, dim=0)\n",
    "    revert = torch.argsort(shuffle, dim=0)\n",
    "    max_seq_len = len(revert) if len(revert)>max_seq_len else max_seq_len\n",
    "    revert_li.append(revert)\n",
    "\n",
    "    # Get keep value\n",
    "    idx_keep = shuffle[:num_sample]\n",
    "    keep_li.append(idx_keep)\n",
    "\n",
    "# Padding\n",
    "arr = torch.nn.utils.rnn.pad_sequence(arr_li, batch_first=True)\n",
    "revert = torch.nn.utils.rnn.pad_sequence(revert_li, batch_first=True, padding_value=max_seq_len-1)\n",
    "keep = torch.nn.utils.rnn.pad_sequence(keep_li, batch_first=True, padding_value=max_seq_len-1)\n",
    "print(\"arr:\\n\", arr)\n",
    "print(\"revert:\\n\", revert)\n",
    "print(\"keep:\\n\", keep)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# Select\n",
    "# Todo: Linear trnasform & pos_encoding\n",
    "masked_input = torch.gather(arr, index=keep, dim=-1)\n",
    "print(\"masked_input:\\n\", masked_input)\n",
    "\n",
    "# Revert\n",
    "filled_input = torch.concat([masked_input, torch.zeros(masked_input.shape[0], (max_seq_len - masked_input.shape[1]+1))], dim=1)\n",
    "print(\"filled_input:\\n\", filled_input)\n",
    "print()\n",
    "torch.gather(filled_input, index=revert, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
